# Crater RGB-D Data Capture and Stitching Pipeline

This repository contains a ROS-based pipeline for capturing synchronized RGB, depth, and pose data of any object of interest using a RealSense camera, and reconstructing textured 3D meshes via Open3D's Colored ICP. This assumes that RGB, Depth and IMU data is being published to a certain ROS topic. This code subscribes to those specific ROS topics to capture RGB, Depth and IMU data to stitch together point clouds.  

---

## Prerequisites

- ROS 1 (tested on Noetic)
- Python 3.8+
- Install required Python packages:

```bash
pip install open3d numpy scipy matplotlib opencv-python pyyaml ros_numpy
```

---

### ROS Setup

This project relies on ROS (Robot Operating System) for data capture and synchronization. Ensure you have ROS installed and properly configured on your system. The following steps outline the ROS setup:

1. **Install ROS**:
   - Follow the official ROS installation guide for your Ubuntu version. For example, for ROS Noetic on Ubuntu 20.04:
     ```bash
     sudo apt update
     sudo apt install ros-noetic-desktop-full
     ```

2. **Source ROS**:
   - Add the ROS setup script to your shell configuration file (e.g., `.bashrc` or `.zshrc`):
     ```bash
     echo "source /opt/ros/noetic/setup.bash" >> ~/.bashrc
     source ~/.bashrc
     ```

3. **Install ROS Dependencies**:
   - Use `rosdep` to install any missing dependencies:
     ```bash
     sudo apt install python3-rosdep
     rosdep init
     rosdep update
     ```

4. **Verify ROS Installation**:
   - Test your ROS installation by running:
     ```bash
     roscore
     ```

5. **Install Additional ROS Packages**:
   - Ensure the following ROS packages are installed:
     ```bash
     sudo apt install ros-noetic-geometry-msgs ros-noetic-sensor-msgs ros-noetic-std-msgs ros-noetic-std-srvs
     ```
---

## How to Use

### Step 1: Start 5 Scripts in Separate Terminals

These scripts will run as ROS nodes:

```bash
# Terminal 1
python3 extract_depth_images.py --live --out depth_output

# Terminal 2
python3 extract_rgb_images.py --live --out rgb_output

# Terminal 3
python3 extract_extrinsics.py --live --out pose_output

# Terminal 4 (only needed once per camera setup)
python3 extract_intrinsics.py --live --out intrinsics_output

# Terminal 5
python3 extract_service_control.py
```

---

### Step 2: Start and Stop Capture

To begin synchronized data capture across all three extractors:

```bash
rosservice call /start_capture_all "data: '<object_name>'"
```

Example:
```bash
rosservice call /start_capture_all "data: 'Crater_1'"
```  
Here we are using a drone equipped with a realsense camera to capture image of a crater on the ground.  


Let the system run for the duration of your data collection.

Then stop all capture:

```bash
rosservice call /stop_capture_all
```

---

## Output Folder Structure

After running, the data will be organized as:

```
depth_output/<object_name>/
    depth_fixed_000.png
    depth_planar_000.npy
    depth_vis_000.png
    depth_timestamps.npy

rgb_output/<object_name>/
    rgb_000.png
    rgb_001.png
    ...

pose_output/<object_name>/
    pose_log.npy

intrinsics_output/
    camera_intrinsics.yaml
```

---

## Next Steps After Capture

1. Align poses to depth timestamps:

```bash
python align_extrinsics.py   --depth-ts depth_output/<object_name>/depth_timestamps.npy   --pose-log pose_output/<object_name>/pose_log.npy   --out-dir extrinsics_output
```

2. Generate Open3D trajectory JSON:

```bash
python trajectory.py   --extrinsics extrinsics_output/<object_name>/extrinsics_matrices.npy   --intrinsics intrinsics_output/camera_intrinsics.yaml   --output trajectory_<object_name>.json
```

3. Run Colored ICP stitching:

```bash
python stitcher.py   --trajectory trajectory_<object_name>.json   --rgb_dir rgb_output/<object_name>   --depth_dir depth_output/<object_name>
```

---

## Final Folder Structure After Alignment & Stitching

After running the `align_extrinsics.py`, `trajectory.py`, and `stitcher.py` scripts, your folder structure will look like this:

```
depth_output/<object_name>/
    depth_fixed_000.png
    depth_planar_000.npy
    depth_vis_000.png
    depth_timestamps.npy

rgb_output/<object_name>/
    rgb_000.png
    rgb_001.png
    ...

pose_output/<object_name>/
    pose_log.npy

intrinsics_output/
    camera_intrinsics.yaml

extrinsics_output/<object_name>/
    extrinsics_data.npy
    extrinsics_matrices.npy

trajectory_<object_name>.json                 # Open3D-compatible camera path
stitched_mesh_<object_name>.ply               # Final textured mesh
```

### Where Each File Comes From

| File | Generated By |
|------|--------------|
| `extrinsics_matrices.npy`, `extrinsics_data.npy` | `align_extrinsics.py` |
| `trajectory_<crater>.json` | `trajectory.py` |
| `stitched_mesh_<crater>.ply` | `stitcher.py` |

---

## Summary

- Run 5 scripts in parallel (RGB, depth, pose, intrinsics, controller)
- Trigger start/stop using `/start_capture_all` and `/stop_capture_all`
- Each extractor saves data into its own `..._output/<object_name>/` folder
- Use the alignment + trajectory + stitching scripts for final 3D mesh